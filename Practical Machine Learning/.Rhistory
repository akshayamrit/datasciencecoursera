as.data.frame(galton$child, galton$parent)
as.data.frame(as.table(galton))
as.data.frame(as.table(galton$child, galton$parent))
as.table(galton)
as.table(galton$child)
head(galton)
swirl()
fit <- lm(child~parent, galton)
summary(fit)
mean(fit$residuals)
cov(fit$residuals, galton)
cov(fit$residuals, galton$parent)
ols.ic <- fit$coefficients[1]
ols.ic <- fit$coeff[1]
ols.ic <- fit$coef[1]
ols.ic <- fit$coef[2]
ols.ic <- fit$coef[1]
ols.slope <- fit$coef[2]
lhs - rhs
all.equal(lhs, rhs)
varChild <- var(galton$child)
varRes <- var(fit$residuals)
varEst <- est(ols.slope, ols.ic)
varEst <- var(est(ols.slope, ols.ic))
all.equal(varChild, sum(varRes, varEst))
all.equal(varChild, varRes+varEst)
efit <- lm(accel~mag+dist, attenu)
mean(efit$residuals)
cov(efit$residuals, attenu$mag)
cov(efit$residuals, attenu$dist)
rm(list = ls())
x <- c(0.18, -1.54, 0.42, 0.95)
w <- c(2, 1, 3, 1)
q1 <- lm(x,w)
q1 <- lm(x~w)
summary(q1)
q1$coefficients
swirl()
cor(gpa_nor, gch_nor)
l_nor <- lm(gch~gpr)
l_nor <- lm(gch~gpa)
l_nor <- lm(gch_nor~gpa_nor)
rm(list = ls())
x <- c(0.18, -1.54, 0.42, 0.95)
w <- c(2, 1, 3, 1)
mean(x)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
cor(y,x)
cor(y~x)
summary(lm(y,x))
summary(lm(y~x))
cor(x,y)
l_2 <- lm(y~x)
l_2$coefficients
data("mtcars")
l_3 <- lm(mtcars$mpg~mtcars$wt)
l_3$coefficients
x <- c(8.58, 10.46, 9.01, 9.64, 8.86)
xnorm <- (x - mean(x))/sd(x)
xnorm
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
l_2$model
l_2$fitted.values
1.5 * 4
1.5*0.4
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
mean(x)
x <- c(0.18, -1.54, 0.42, 0.95)
w <- c(2, 1, 3, 1)
sum(x*w)/sum(w)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
lreg <- lm(y~x-1)
summary(lreg)
data(mtcars)
summary(lm(mtcars$mpg, mtcars$wt))
summary(lm(mtcars$mpg~mtcars$wt))
x <- c(8.58, 10.46, 9.01, 9.64, 8.86)
(x - mean(x)) / sd(x)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
lreg <- lm(y~x)
summary(lreg)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
mean(x)
rm(list = ls())
x <- galton$parent
y <- galton$child
as.data.frame(as.table(x,y))
as.data.frame(table(x,y))
?as.table
?table
rm(list = ls())
library(UsingR)
data("diamond")
diamond
g <- ggplot(data = diamond, aes(x = carat, y = price))
g <- g + geom_point(size = 2, colour = "black", alpha = 0.2)
g
g <- g + geom_point(size = 5, colour = "blue", alpha = 0.2)
g
g <- g + geom_smooth(method = "lm", colour = "red")
g
# make this an external chunk that can be included in any file
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')
plot(diamond$carat, e,
xlab = "Mass (carats)",
ylab = "Residuals (SIN $)",
bg = "lightblue",
col = "black", cex = 1.1, pch = 21,frame = FALSE)
x <- runif(100, 0, 6); y <- x + rnorm(100,  mean = 0, sd = .001 * x);
plot(x, y); abline(lm(y ~ x))
x <- runif(100, 0, 6); y <- x + rnorm(100,  mean = 0, sd = .001 * x);
plot(x, y); abline(lm(y ~ x))
x <- runif(100, -3, 3); y <- x + sin(x) + rnorm(100, sd = .2);
plot(x, y); abline(lm(y ~ x))
x <- runif(100, -3, 3); y <- x + sin(x) + rnorm(100, sd = .2);
plot(x, y, col = "red"); abline(lm(y ~ x))
x <- runif(100, -3, 3); y <- x + sin(x) + rnorm(100, sd = .2);
plot(x, y, col = "black", fill = red); abline(lm(y ~ x))
x <- runif(100, -3, 3); y <- x + sin(x) + rnorm(100, sd = .2);
plot(x, y, col = "black", fill = "red"); abline(lm(y ~ x))
x <- runif(100, -3, 3); y <- x + sin(x) + rnorm(100, sd = .2);
plot(x, y, col = "red"); abline(lm(y ~ x))
warnings()
g
g <- ggplot(data = diamond, aes(x = diamond$carat, y = diamond$price))
g + geom_point()
g + geom_point(colour = "blue", size = 2, aplha = 0.2)
g + geom_point(colour = "blue", size = 2, alpha = 0.2)
g + geom_point(colour = "blue", size = 2, alpha = 0.2) + geom_hline(lm(diamond$price~diamond$carat))
g + geom_point(colour = "blue", size = 2, alpha = 0.2) + geom_hline()
g + geom_point(colour = "blue", size = 2, alpha = 0.2) + geom_hline(yintercept = 0)
g <- ggplot(data = diamond, aes(x = diamond$carat, y = resid(lm(diamond$price~diamond$carat))))
g
g + geom_hline(yintercept = 0) + geom_point(colour = "blue", size = 3)
rm(list = ls())
swirl()
fit <- lm(child~parent, data = galton)
sqrt(sum(fit$residuals^2/n-1))
sqrt(sum(fit$residuals^2)/(n-2))
summary(fit)$sigma
sqrt(deviance(fit)/(n-2))
mu <- mean(galton$child)
sum((galton$child - mu)^2)
sTot <- sum((galton$child-mu)^2)
sRes <- deviance(sum(fit$residuals^2))
sRes <- deviance(sum((fit$residuals)^2))
sRes <- deviance(sum((fit$residuals))
)
sRes <- deviance(galton$child^2)
sRes <- deviance(sTot)
sRes <- deviance(galton)
sRes <- deviance(fit)
1 - (sRes/sTot)
1-sRes/sTot
summary(fit)$r.squared
cor(galton$child, galton$parent)^2
0
quit
quit()
hi
rm()
0
quit()
back()
swirl::bye()
library(swirl)
swirl()
ones <- rep(1, nrow(galton))
lm(child~ones + parent - 1, galton)
lm(child~parent, galton)
lm(child~1, galton)
View(trees)
fit <- lm(Volume ~ Girth + Height + Constant - 1, trees)
trees2 <- eliminate("Girth", trees)
View(trees2)
fit2 <- lm(Volume ~ Height + Constant - 1, trees2)
lapply(list(fit, fit2), coef)
all <- lm(Fertility ~ ., swiss)
summary(all)
summary(lm(Fertility ~ Agriculture, swiss))
cor(swiss$Examination, swiss$Education)
cor(swiss$Agriculture, swiss$Education)
makelms()
sum(swiss$Examination,swiss$Catholic)
ec <- swiss$Examination+swiss$Catholic
efit <- lm(Fertility ~ . + ec, swiss)
all$coefficients - efit$coefficients
rm(list = ls())
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
summary(lm(y~x))
data("mtcars")
summary(lm(mpg~weight, mtcars))
summary(lm(mtcars$mpg~mtcars$wt))
>mtcars
?mtcars
library(UsingR)
y <- galton$child
# regressor
x <- galton$parent
summary(lm(y~x))
ggplot(data = as.data.frame(x,y), aes(x,y)) + geom_abline(lm(y~x))
?predict
predict(lm(y~x), interval = "confidence")
summary(lm(y~x))
data("mtcars")
fit <- lm(mtcars$mpg~mtcars$wt)
predict(fit, data.frame(wt = mean(mtcars$wt)), interval = "confidence")
confint(fit)
predict.lm(fit, newdata = data.frame(wt = 3))
data("cars")
ggplot(cars, aes(x=speed, y=dist)) +
geom_point(color='#2980B9', size = 4) +
geom_smooth(method=lm, color='#2C3E50')
cars.lm <- lm(dist ~ speed, data = cars)
summary(cars.lm)
new.dat <- data.frame(speed=30)
predict(cars.lm, newdata = new.dat, interval = 'confidence')
View(cars)
head(mtcars)
new.dat <- data.frame(wt = 3)
predict(fit, newdata = new.dat, interval = "confidence")
confint.lm(fit)
predict(fit, new.dat)
predict(fit, new.dat, interval = "predict")
predict(fit, interval = "predict")
fit <- lm(mpg ~ wt, data = mtcars)
predict(fit, newdata = data.frame(wt = 3), interval = "prediction")
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
summary(fit)
fit <- lm(y ~ x)
summary(fit)
y <- mtcars$mpg
x <- mtcars$wt
fit_car <- lm(y ~ x)
predict(fit_car, newdata = data.frame(x = mean(x)), interval = ("confidence"))
summary(fit_car)
rm(list = ls())
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
fit <- lm(y~x)
summary(fit)
data("mtcars")
fit_cars <- lm(mpg~wt, data = mtcars)
predict(fit, newdata = data.frame(wt = mean(mtcars$wt)), interval = "confidence")
predict(fit_cars, newdata = data.frame(wt = mean(mtcars$wt)), interval = "confidence")
predict(fit_cars, newdata = data.frame(wt = 3), interval = "predict")
fit_cars2 <- lm(mpg~I(wt/2), data = mtcars)
predict(fit_cars2, newdata = data.frame(wt = 1), interval = "predict")
predict(fit_cars2, newdata = data.frame(wt = 1), interval = "confidence")
sumCoef2 <- coef(summary(fit_car2))
(sumCoef2[2,1] + c(-1, 1) * qt(.975, df = fit_car2$df) * sumCoef2[2, 2])
sumCoef2 <- coef(summary(fit_cars2))
(sumCoef2[2,1] + c(-1, 1) * qt(.975, df = fit_cars2$df) * sumCoef2[2, 2])
sumCoef2
sumCoef2[2,1]
sumCoef2[2,2]
fit <- lm(y~x)
fit2 <- lm(y~I(x/100))
coef(fit)
coef(fit2)
require(datasets)
data("swiss")
head(swiss)
library(ggplot2)
require(GGally)
install.packages("GGally")
library(GGally)
ggpairs(data = swiss, lower = list(continuous = "smooth"), params = c(method = "loess"))
g <- ggpairs(data = swiss, lower = list(continuous = "smooth"), params = c(method = "loess"))
summary(lm(Fertility ~ ., data = swiss))
summary(lm(Fertility ~ Agriculture, data = swiss))
rm(list = ls())
library(swirl)
setwd("~/datasciencecoursera/Practical Machine Learning")
library(caret)
library(AppliedPredictiveModeling)
library(Hmisc)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
names <- colnames(concrete)
names
names <- names[-length(names)]
names
featurePlot(x = training[, names], y = training$CompressiveStrength, plot = "pairs")
index <- seq_along(1:nrow(training))
index
qplot(x = index, y = CompressiveStrength, data = training)
ggplot(data = training, aes(x = index, y = CompressiveStrength)) + geom_point()
suppressMessages(library(dplyr))
suppressMessages(library(Hmisc))
suppressMessages(library(gridExtra))
training <- mutate(training, index=1:nrow(training))
cutIndex <- cut2(training$index, g=10)
breaks <- 10
#byCement <- qplot(index, CompressiveStrength, data=training, color=cut2(training$Cement, g=breaks))
#byBlastFurnaceSlag <- qplot(index, CompressiveStrength, data=training, color=cut2(training$BlastFurnaceSlag, g=breaks))
#byFlyAsh <- qplot(index, CompressiveStrength, data=training, color=cut2(training$FlyAsh, g=breaks))
#byWater <- qplot(index, CompressiveStrength, data=training, color=cut2(training$Water, g=breaks))
#bySuperplasticizer <- qplot(index, CompressiveStrength, data=training, color=cut2(training$Superplasticizer, g=breaks))
#byCoarseAggregate <- qplot(index, CompressiveStrength, data=training, color=cut2(training$CoarseAggregate, g=breaks))
#byFineAggregate <- qplot(index, CompressiveStrength, data=training, color=cut2(training$FineAggregate, g=breaks))
#byAge <- qplot(index, CompressiveStrength, data=training, color=cut2(training$Age, g=breaks))
#grid.arrange(byCement, byBlastFurnaceSlag, byFlyAsh, byWater, bySuperplasticizer, byCoarseAggregate, byFineAggregate, byAge)
qplot(index, CompressiveStrength, data=training, color=cut2(training$Cement, g=breaks))
byCement <- qplot(index, CompressiveStrength, data=training, color=cut2(training$Cement, g=breaks))
byBlastFurnaceSlag <- qplot(index, CompressiveStrength, data=training, color=cut2(training$BlastFurnaceSlag, g=breaks))
byFlyAsh <- qplot(index, CompressiveStrength, data=training, color=cut2(training$FlyAsh, g=breaks))
byWater <- qplot(index, CompressiveStrength, data=training, color=cut2(training$Water, g=breaks))
bySuperplasticizer <- qplot(index, CompressiveStrength, data=training, color=cut2(training$Superplasticizer, g=breaks))
byCoarseAggregate <- qplot(index, CompressiveStrength, data=training, color=cut2(training$CoarseAggregate, g=breaks))
byFineAggregate <- qplot(index, CompressiveStrength, data=training, color=cut2(training$FineAggregate, g=breaks))
byAge <- qplot(index, CompressiveStrength, data=training, color=cut2(training$Age, g=breaks))
grid.arrange(byCement, byBlastFurnaceSlag, byFlyAsh, byWater, bySuperplasticizer, byCoarseAggregate, byFineAggregate, byAge)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
qplot(training$Superplasticizer, data = training, geom = "hist")
?qplot
qplot(training$Superplasticizer, data = training, geom = "histogram")
qplot(log(Superplasticizer+1), data = training, geom = "histogram")
qplot(log(Superplasticizer), data = training, geom = "histogram")
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
names <- colnames(training)
names[grepl("IL$", names)]
sum(grepl("IL$", names))
grep("IL", names)
names[grep("IL", names)]
names[names == "IL$"]
names[names = "IL$"]
library(stringr)
stringr::str_locate(names, "IL$")
stringr::str_match(names, "IL")
stringr::str_match(names, "IL$")
stringr::str_which(names, "IL")
stringr::str_which(names, "IL$")
startsWith(names, "IL")
training[, startsWith(names, "IL")]
table(startsWith(names, "IL"))
preProcess(training[, startsWith(names, "IL")], method = c("center", "scale", "pca"), thresh = 0.8)
rm(list = ls())
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
testing = adData[-inTrain,]
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)
[[1]]training = adData[ inTrain,]
testing = adData[-inTrain,]
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
training.subset <- training %>% select(diagnosis, names(training)[startsWith(colnames(training), "IL")])
head(training.subset)
dim(training.subset)
model1 <- train(training.subset$diagnosis~., method = "glm", data = training.subset)
model1 <- train(diagnosis~., method = "glm", data = training.subset)
pre2 <- preProcess(training.subset[, -1], method = "pca", thresh = 0.8)
model2 <- predict(pre2, training.subset[,-1])
test1 <- predict(model1, testing[, startsWith(colnames(testing), "IL")])
test2 <- predict(pre2, testing[, startsWith(colnames(testing), "IL")])
rm(list = ls())
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
training <- training %>% select(diagnosis, names(training)[startsWith(colnames(training), "IL")])
testing <- testing %>% select(diagnosis, names(testing)[startsWith(colnames(testing), "IL")])
model1 <- train(diagnosis~., method = "glm", data = training)
pre2 <- preProcess(training[,-1], method = "pca", thresh = 0.8)
trainPC <- predict(pre2, training[,-1])
model2 <- train(training$diagnosis~., method = "glm", data = trainPC)
model2 <- train(diagnosis~., method = "glm", data = trainPC)
trainPC
model2 <- train(training$diagnosis~., method = "glm", data = trainPC)
rm(list = ls())
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
training <- training %>% select(diagnosis, names(training)[startsWith(colnames(training), "IL")])
testing <- testing %>% select(diagnosis, names(testing)[startsWith(colnames(testing), "IL")])
model1 <- train(training$diagnosis~., data = training, method = "glm")
model1 <- train(diagnosis~., data = training, method = "glm")
non_pca_result <- confusionMatrix(testing[,1], predict(model1, testing[,-1]))
non_pca_result
pc_training_obj <- preProcess(training[,-1], method = c("center", "scale", "pca"), thresh = 0.8)
pc_train_pred <- predict(pc_training_obj, training[,-1])
model2 <- train(training$diagnosis~., data = pc_train_pred, method = "glm")
model2 <- train(diagnosis~., data = pc_train_pred, method = "glm")
rm(list = ls())
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
# extract new training and testing sets
IL_col_idx <- grep("^[Ii][Ll].*", names(training))
suppressMessages(library(dplyr))
new_training <- training[, c(names(training)[IL_col_idx], "diagnosis")]
names(new_training)
IL_col_idx <- grep("^[Ii][Ll].*", names(testing))
suppressMessages(library(dplyr))
new_testing <- testing[, c(names(testing)[IL_col_idx], "diagnosis")]
names(new_testing)
# compute the model with non_pca predictors
non_pca_model <- train(diagnosis ~ ., data=new_training, method="glm")
# apply the non pca model on the testing set and check the accuracy
non_pca_result <- confusionMatrix(new_testing[, 13], predict(non_pca_model, new_testing[, -13]))
non_pca_result
# perform PCA extraction on the new training and testing sets
pc_training_obj <- preProcess(new_training[, -13], method=c('center', 'scale', 'pca'), thresh=0.8)
pc_training_preds <- predict(pc_training_obj, new_training[, -13])
pc_testing_preds <- predict(pc_training_obj, new_testing[, -13])
# compute the model with pca predictors
pca_model <- train(new_training$diagnosis ~ ., data=pc_training_preds, method="glm")
# apply the PCA model on the testing set
pca_result <- confusionMatrix(new_testing[, 13], predict(pca_model, pc_testing_preds))
pca_result
rm(list = ls())
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
trainSmall <- data.frame(training[,grep('^IL',names(training))],training$diagnosis)
testSmall <- data.frame(testing[,grep('^IL',names(testing))],testing$diagnosis)
preProc <- preProcess(trainSmall[-13],method="pca",thres=.8)
trainPC <- predict(preProc,trainSmall[-13])
testPC <- predict(preProc,testSmall[-13])
PCFit <- train(trainSmall$training.diagnosis~.,data=trainPC,method="glm")
NotPCFit <- train(trainSmall$training.diagnosis~.,data=trainSmall,method="glm")
PCTestPredict <- predict(PCFit,newdata=testPC)
NotPCTestPredict <- predict(NotPCFit,newdata=testSmall)
confusionMatrix(PCTestPredict,testSmall$testing.diagnosis)
rm(list = ls())
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(3430)
suppressMessages(library(dplyr))
IdxCol_IL <- grep("^IL", names(testing))
names_IL <- names(testing[,IdxCol_IL])
newcols <- c(names_IL,"diagnosis")
new_testing <- testing [,newcols]
new_training <- training[,newcols]
# Model 1 : predictors as they are, without PCA
model_without_PCA <- train(diagnosis~., data=new_training,   preProcess=c("center","scale"),method="glm")
model_result_without_PCA <- confusionMatrix(new_testing$diagnosis,predict(model_without_PCA,subset(new_testing, select = -c(diagnosis))))
model_result_without_PCA
# Model 2 : predictors using PCA, with explanation of 80% of variance
preProc_pca <-  preProcess(subset(new_training, select = -c(diagnosis)), method="pca", thresh=0.8)
trainPC <- predict(preProc_pca,subset(new_training, select = -c(diagnosis)))
testPC <- predict(preProc_pca,subset(new_testing, select = -c(diagnosis)))
# Syntax to use to avoid "undefined columns selected" error message (by following the formula  defined in the slides.)
model_with_PCA<- train(x = trainPC, y = new_training$diagnosis,method="glm")
model_result_with_PCA <- confusionMatrix(new_testing$diagnosis,predict(model_with_PCA, newdata=testPC))
model_result_with_PCA
rm(list = ls())
